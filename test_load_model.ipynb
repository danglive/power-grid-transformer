{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792f7259-fde8-4c53-9002-855b0a375e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 13:26:53,602 - src.data - INFO - Loaded Q-Transformer data module with 23 components\n",
      "2025-04-18 13:26:53,604 - src.data.qtransformer_data - INFO - Using action weights: False\n",
      "2025-04-18 13:26:53,604 - src.data.qtransformer_data - INFO - Loading training data from 1 files...\n",
      "2025-04-18 13:26:53,605 - src.data.qtransformer_data - INFO - Loading file: train_val.npz\n",
      "2025-04-18 13:27:09,397 - src.data.enhanced_splitter - INFO - Splitting 50376 samples with strategy: stratified_temporal\n",
      "2025-04-18 13:27:14,279 - src.data.enhanced_splitter - INFO - === Split Summary ===\n",
      "2025-04-18 13:27:14,282 - src.data.enhanced_splitter - INFO - Train samples: 42840\n",
      "2025-04-18 13:27:14,283 - src.data.enhanced_splitter - INFO - Val samples:   7536\n",
      "2025-04-18 13:27:14,283 - src.data.enhanced_splitter - INFO - Test samples:  0\n",
      "2025-04-18 13:27:14,284 - src.data.enhanced_splitter - INFO - Train time: 2021-01-04T07:45:00.000000 → 2023-12-07T12:55:00.000000\n",
      "2025-04-18 13:27:14,285 - src.data.enhanced_splitter - INFO - Val time:   2021-09-27T14:40:00.000000 → 2023-12-22T11:30:00.000000\n",
      "2025-04-18 13:27:14,286 - src.data.enhanced_splitter - INFO - JSD Drift (train vs val label dist): 0.0000\n",
      "2025-04-18 13:27:14,287 - src.data.enhanced_splitter - INFO - JSD Drift (train vs val action dist): 0.0097\n",
      "2025-04-18 13:27:14,288 - src.data.enhanced_splitter - INFO - Rate of unseen labels in val:       0.00%\n",
      "2025-04-18 13:27:14,289 - src.data.enhanced_splitter - INFO - Maximum weekly JSD: 0.6399 (week 2023-07-06)\n",
      "2025-04-18 13:27:14,290 - src.data.enhanced_splitter - INFO - ⚠️  2.83% of val samples are from future relative to train\n",
      "2025-04-18 13:27:14,290 - src.data.enhanced_splitter - INFO -    Future val time range: 2023-12-07T13:00:00.000000 → 2023-12-22T11:30:00.000000\n",
      "2025-04-18 13:27:14,291 - src.data.enhanced_splitter - INFO - Note: No test data in this split (use external test file if needed)\n",
      "2025-04-18 13:27:16,067 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-04-18 13:27:16,071 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-04-18 13:27:16,544 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-04-18 13:27:16,553 - matplotlib.category - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-04-18 13:27:17,240 - src.data.enhanced_splitter - INFO - Saved split report to data_cache/visualizations/split_report.json\n",
      "2025-04-18 13:27:17,241 - src.data.qtransformer_data - INFO - Creating training dataset...\n",
      "2025-04-18 13:27:17,253 - src.data.qtransformer_data - INFO - Dataset dimensions - Observations: 3819, Actions: 50, Action vector: 1152\n",
      "2025-04-18 13:27:17,254 - src.data.qtransformer_data - INFO - Prefetching data for faster access...\n",
      "2025-04-18 13:27:17,560 - src.data.qtransformer_data - INFO - Data prefetching completed in 0.31 seconds\n",
      "2025-04-18 13:27:17,561 - src.data.qtransformer_data - INFO - Action cache stats: {'size': 50, 'capacity': 10000, 'usage': 0.005, 'access_count': 50000}\n",
      "2025-04-18 13:27:17,562 - src.data.qtransformer_data - INFO - Soft label cache stats: {'size': 50, 'capacity': 10000, 'usage': 0.005, 'access_count': 50000}\n",
      "2025-04-18 13:27:17,563 - src.data.qtransformer_data - INFO - Creating validation dataset...\n",
      "2025-04-18 13:27:17,565 - src.data.qtransformer_data - INFO - Dataset dimensions - Observations: 3819, Actions: 50, Action vector: 1152\n",
      "2025-04-18 13:27:17,566 - src.data.qtransformer_data - INFO - Prefetching data for faster access...\n",
      "2025-04-18 13:27:17,861 - src.data.qtransformer_data - INFO - Data prefetching completed in 0.29 seconds\n",
      "2025-04-18 13:27:17,862 - src.data.qtransformer_data - INFO - Action cache stats: {'size': 50, 'capacity': 10000, 'usage': 0.005, 'access_count': 50000}\n",
      "2025-04-18 13:27:17,863 - src.data.qtransformer_data - INFO - Soft label cache stats: {'size': 50, 'capacity': 10000, 'usage': 0.005, 'access_count': 50000}\n",
      "2025-04-18 13:27:17,864 - src.data.qtransformer_data - INFO - Loading test data...\n",
      "2025-04-18 13:27:17,865 - src.data.qtransformer_data - INFO - Loading testing data from 1 files...\n",
      "2025-04-18 13:27:17,866 - src.data.qtransformer_data - INFO - Loading file: test.npz\n",
      "2025-04-18 13:27:19,909 - src.data.qtransformer_data - INFO - External test dataset contains 6614 samples\n",
      "2025-04-18 13:27:19,911 - src.data.qtransformer_data - INFO - Test time range: 2024-05-16T13:35:00.000000 → 2024-12-31T23:50:00.000000\n",
      "2025-04-18 13:27:19,912 - src.data.qtransformer_data - INFO - Creating test dataset...\n",
      "2025-04-18 13:27:19,913 - src.data.qtransformer_data - INFO - Dataset dimensions - Observations: 3819, Actions: 50, Action vector: 1152\n",
      "2025-04-18 13:27:19,914 - src.data.qtransformer_data - INFO - Prefetching data for faster access...\n",
      "2025-04-18 13:27:20,218 - src.data.qtransformer_data - INFO - Data prefetching completed in 0.30 seconds\n",
      "2025-04-18 13:27:20,219 - src.data.qtransformer_data - INFO - Action cache stats: {'size': 50, 'capacity': 10000, 'usage': 0.005, 'access_count': 50000}\n",
      "2025-04-18 13:27:20,220 - src.data.qtransformer_data - INFO - Soft label cache stats: {'size': 50, 'capacity': 10000, 'usage': 0.005, 'access_count': 50000}\n",
      "2025-04-18 13:27:20,221 - src.data.qtransformer_data - INFO - Computing dataset statistics...\n",
      "2025-04-18 13:27:20,222 - src.data.base - INFO - Computing dataset statistics...\n",
      "2025-04-18 13:27:26,640 - src.data.base - INFO - Statistics computation completed in 6.42 seconds\n",
      "2025-04-18 13:27:26,642 - src.data.base - INFO - Dataset Statistics:\n",
      "2025-04-18 13:27:26,643 - src.data.base - INFO -   Number of samples: 50376\n",
      "2025-04-18 13:27:26,644 - src.data.base - INFO -   Observation shape: torch.Size([3819])\n",
      "2025-04-18 13:27:26,644 - src.data.base - INFO -   Action dimension: 1152\n",
      "2025-04-18 13:27:26,645 - src.data.base - INFO -   Actions per sample: 50\n",
      "2025-04-18 13:27:26,646 - src.data.base - INFO -   Feature range: [-316312.1250, 399814721536.0000]\n",
      "2025-04-18 13:27:26,647 - src.data.base - INFO -   Feature mean range: [-316258.0312, 399837429760.0000]\n",
      "2025-04-18 13:27:26,648 - src.data.base - INFO -   Rho range: [0.3369, 6.4651]\n",
      "2025-04-18 13:27:26,649 - src.data.base - INFO -   Rho mean/std: 1.1505/0.2856\n",
      "2025-04-18 13:27:26,652 - src.data.base - INFO - Statistics cached to data_cache/dataset_stats.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: ['observation', 'action_vectors', 'rho_values', 'soft_labels', 'action_weights', 'timestep', 'idx', 'best_action']\n",
      "Observations shape: torch.Size([128, 3819])\n",
      "Action vectors shape: torch.Size([128, 50, 1152])\n",
      "Rho values shape: torch.Size([128, 50])\n",
      "Soft labels shape: torch.Size([128, 50])\n",
      "action_weights: tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "\n",
      "Model structure:\n",
      "PowerGridModel(\n",
      "  (observation_encoder): ObservationEncoder(\n",
      "    (input_projection): Linear(in_features=3819, out_features=1024, bias=True)\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (mha): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (wk): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (wv): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ffn0): PointWiseFeedForwardNetwork(\n",
      "          (linear1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ffn1): PointWiseFeedForwardNetwork(\n",
      "          (linear1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        )\n",
      "        (rz0): ReZero()\n",
      "        (rz1): ReZero()\n",
      "        (rz2): ReZero()\n",
      "        (layernorm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (action_encoder): ActionEncoder(\n",
      "    (input_projection): Linear(in_features=1152, out_features=1024, bias=True)\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (mha): MultiHeadAttention(\n",
      "          (wq): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (wk): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (wv): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ffn0): PointWiseFeedForwardNetwork(\n",
      "          (linear1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        )\n",
      "        (ffn1): PointWiseFeedForwardNetwork(\n",
      "          (linear1): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        )\n",
      "        (rz0): ReZero()\n",
      "        (rz1): ReZero()\n",
      "        (rz2): ReZero()\n",
      "        (layernorm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (layernorm3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_attention): CrossAttention(\n",
      "    (wq): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (wk): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (wv): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (rho_head): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (soft_label_head): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (rho_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "  (softlabel_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "  (rho_activation): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "\n",
      "Running forward pass...\n",
      "\n",
      "Output shapes:\n",
      "rho_values shape: torch.Size([128, 50])\n",
      "soft_labels shape: torch.Size([128, 50])\n",
      "\n",
      "Loss values:\n",
      "loss: 1.2520\n",
      "rho_loss: 1.4059\n",
      "soft_label_loss: 1.0981\n",
      "\n",
      "Metrics:\n",
      "top_1_accuracy: 0.0156\n",
      "top_3_accuracy: 0.0234\n",
      "recall@3: 0.6745\n",
      "precision@3: 0.6745\n",
      "ndcg@3: 0.5417\n",
      "top_5_accuracy: 0.0234\n",
      "recall@5: 0.8047\n",
      "precision@5: 0.8047\n",
      "ndcg@5: 0.8574\n",
      "rho_mae: 1.1456\n",
      "rho_mse: 1.4059\n",
      "rho_rmse: 1.1857\n",
      "rho_r_squared: -17.5092\n",
      "rho_pearson: 0.0000\n",
      "rho_spearman: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "# Import module of  model\n",
    "from src.models.observation_encoder import ObservationEncoder\n",
    "from src.models.action_encoder import ActionEncoder\n",
    "from src.models.cross_attention import CrossAttention\n",
    "from src.models.model import PowerGridModel\n",
    "\n",
    "# Import DataModule to get batch data\n",
    "from src.data import QTransformerDataModule\n",
    "\n",
    "# Load config\n",
    "import json\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Khởi tạo DataModule\n",
    "data_module = QTransformerDataModule(config)\n",
    "data_module.setup()\n",
    "\n",
    "# Get a batch from train loader\n",
    "train_loader = data_module.get_train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Print information\n",
    "print(\"Batch keys:\", list(batch.keys()))\n",
    "print(\"Observations shape:\", batch['observation'].shape)\n",
    "print(\"Action vectors shape:\", batch['action_vectors'].shape)\n",
    "print(\"Rho values shape:\", batch['rho_values'].shape)\n",
    "print(\"Soft labels shape:\", batch['soft_labels'].shape)\n",
    "print(\"action_weights:\",batch[\"action_weights\"])\n",
    "\n",
    "# Initialize model\n",
    "model = PowerGridModel(config)\n",
    "\n",
    "# Print model archtecture\n",
    "print(\"\\nModel structure:\")\n",
    "print(model)\n",
    "\n",
    "# Convert to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare input batch\n",
    "input_batch = {\n",
    "    'observation': batch['observation'].to(device),\n",
    "    'action_vectors': batch['action_vectors'].to(device)\n",
    "}\n",
    "\n",
    "# Ground truth for compute loss\n",
    "targets = {\n",
    "    'rho_values': batch['rho_values'].to(device),\n",
    "    'soft_labels': batch['soft_labels'].to(device),\n",
    "    'best_action': batch.get('best_action', None),\n",
    "    'action_weights': batch['action_weights'].to(device)  \n",
    "}\n",
    "\n",
    "# Forward pass\n",
    "print(\"\\nRunning forward pass...\")\n",
    "with torch.no_grad():\n",
    "    predictions = model(input_batch)\n",
    "\n",
    "print(\"\\nOutput shapes:\")\n",
    "for key, tensor in predictions.items():\n",
    "    print(f\"{key} shape: {tensor.shape}\")\n",
    "\n",
    "# Import loss function và metric\n",
    "from src.utils.loss import PowerGridLoss\n",
    "from src.utils.metrics import compute_all_metrics\n",
    "\n",
    "# Tính loss\n",
    "loss_fn = PowerGridLoss(\n",
    "    rho_weight=config['model_params'].get('rho_weight', 0.5),\n",
    "    soft_label_weight=config['model_params'].get('soft_label_weight', 0.5)\n",
    ")\n",
    "\n",
    "loss_dict = loss_fn(predictions, targets)\n",
    "\n",
    "print(\"\\nLoss values:\")\n",
    "for key, value in loss_dict.items():\n",
    "    print(f\"{key}: {value.item():.4f}\")\n",
    "\n",
    "# Compute metrics only \"best_action disponible\n",
    "if 'best_action' in batch and batch['best_action'] is not None:\n",
    "    metrics = compute_all_metrics(predictions, targets)\n",
    "    print(\"\\nMetrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb81d6-7f3d-4cc7-bfe0-85dea0feea9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python assistflux",
   "language": "python",
   "name": "assistflux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
